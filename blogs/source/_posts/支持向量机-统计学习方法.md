---
title: 支持向量机 - 统计学习方法
date: 2016-03-17 16:59:57
tags: 统计学习方法
categories: machine_learning
---

### 7.1 决策函数

**支持向量机分类决策函数**是: $$f(x) = sign(w^* \cdot x + b^*)$$ 对应的分离超平面是 $w^* \cdot x + b^* = 0$

____________________________

### 7.2 线性可分支持向量机的学习策略

1. 数据集线性可分的情况下, 其**学习策略**为硬间隔最大化, 对应的**初始最优化问题**为 $$\min\limits_{w, b} \quad \frac{1}{2} ||w||^2$$ $$s.t. \quad y_i(w \cdot x_i + b)-1 \ge 0 \;, \quad i = 1, 2, \cdots, N$$  支持向量为间隔边界上的实例点.

2. **对偶最优化问题**为 $$\min\limits_{\alpha} \quad \frac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum\limits_{i=1}^{N} \alpha_i$$ $$s.t. \quad \sum\limits_{i=1}^{N} \alpha_i y_i = 0 \quad , \quad \alpha_i \ge 0 \; , \quad i = 1, 2, \cdots, N$$

3. 通常, 首先求解对偶问题的最优值 $\alpha^*$, 然后求最优值 $w^*$ 和 $b^*$ , 得到分离超平面和分类决策函数.

_________

### 7.3 线性支持向量机的学习策略

1. 数据集近似线性可分的情况下, 其**学习策略**为软间隔最大化, 对应的**初始最优化问题**为 $$\min\limits_{w, b, \xi} \quad \frac{1}{2} ||w||^2 + C \sum\limits_{i=1}^{N} \xi_i$$ $$s.t. \quad y_i(w \cdot x_i + b) \ge 1 - \xi_i \;, \quad \xi_i \ge 0 , \quad i = 1, 2, \cdots, N$$  支持向量可在间隔边界上,  也可以在间隔边界与分离超平面之间, 或者在分离超平面误分一侧.

2. **对偶最优化问题**为 $$\min\limits_{\alpha} \quad \frac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum\limits_{i=1}^{N} \alpha_i$$ $$s.t. \quad \sum\limits_{i=1}^{N} \alpha_i y_i = 0 \quad , \quad 0 \le \alpha_i \le C \; , \quad i = 1, 2, \cdots, N$$

_________

### 7.4 非线性支持向量机

1. 对于输入空间中的非线性问题, 可以通过非线性转换将它转化为某个高维特征空间中的线性分类问题, 在高维特征空间中学习线性支持向量机. 

2. 由于在线性支持向量机学习的对偶问题里, 目标函数和分类决策函数都只涉及实例与实例之间的内积, 所以不需要显式地指定非线性变换, 而是用核函数来替换当中的内积.